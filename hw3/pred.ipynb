{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d701d9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/andy/anaconda3/envs/adlhw3/lib/python3.8/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:909: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "/home/data/andy/anaconda3/envs/adlhw3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "[nltk_data] Downloading package punkt to /home/data/andy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import PreTrainedModel\n",
    "from typing import Optional, Union\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from tw_rouge import get_rouge\n",
    "\n",
    "input_path =  \"./input.jsonl\" # 為 input data 的位置\n",
    "output_path = \"./output.jsonl\" # 為 output data 的位置\n",
    "weights_path = \"./weights/\"\n",
    "model_checkpoint = \"google/mt5-small\"\n",
    "nltk.download('punkt')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf23096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with jsonlines.open(path, \"r\") as f:\n",
    "        for row in f:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSeq2Seq:\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    model: Optional[PreTrainedModel] = None\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        features = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "       \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d3b20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading...\n"
     ]
    }
   ],
   "source": [
    "print(\"data loading...\")\n",
    "data = load_jsonl(input_path)\n",
    "\n",
    "test_data = {\"maintext\":[], \"id\":[]}\n",
    "for sub in data:\n",
    "    test_data[\"maintext\"].append(sub[\"maintext\"])\n",
    "    test_data[\"id\"].append(sub[\"id\"])\n",
    "\n",
    "test_datasets = Dataset.from_dict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49c39d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocessing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236f19bb43704898b0c6dadc94e6e1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"data preprocessing...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"maintext\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer(examples[\"title\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = test_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01d0940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loading...\n"
     ]
    }
   ],
   "source": [
    "print(\"model loading...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(weights_path)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378f7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = trainer.get_test_dataloader(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f782c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_length = 128\n",
    "pred_list=[]\n",
    "label_list=[]\n",
    "\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    inputs = batch['input_ids'].to(device)\n",
    "    preds = model.generate(\n",
    "        input_ids=inputs, \n",
    "        attention_mask=attention_mask, \n",
    "        max_length=max_target_length,\n",
    "        num_beams=2,\n",
    "    )\n",
    "    \n",
    "    for p in preds.cpu().numpy():\n",
    "        pred_list.append(p)\n",
    "        \n",
    "decoded_preds = tokenizer.batch_decode(pred_list, skip_special_tokens=True)\n",
    "decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96b0e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(output_path, \"w\") as writer:\n",
    "    for i in range(len(decoded_preds)):\n",
    "        d = {\"title\":decoded_preds[i], \"id\":data[i][\"id\"]}\n",
    "        writer.write(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}